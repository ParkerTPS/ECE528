{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\",\n",
    "        \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\", \n",
    "        \"num_file_creations\", \"num_shells\", \"num_access_files\", \"num_outbound_cmds\", \"is_host_login\",\n",
    "        \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n",
    "        \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\", \n",
    "        \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\",\n",
    "        \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\", \"label\"]\n",
    "\n",
    "# if the name of the data file is \"kddcup.data\" then replace the code below with \"kddcup.data\"\n",
    "df = pd.read_csv(\"kddcup.data.corrected\", sep=\",\", names=columns, index_col=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"service\"] == \"http\"]\n",
    "df = df.drop(\"service\", axis=1)\n",
    "columns.remove(\"service\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if df[col].dtype == \"object\":\n",
    "        encoded = LabelEncoder()\n",
    "        encoded.fit(df[col])\n",
    "        df[col] = encoded.transform(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in range(0, 3):\n",
    "    df = df.iloc[np.random.permutation(len(df))]\n",
    "    \n",
    "df2 = df[:500000]\n",
    "labels = df2[\"label\"]\n",
    "df_validate = df[500000:]\n",
    "x_train, x_test, y_train, y_test = train_test_split(df2, labels, test_size = 0.2, random_state = 42)\n",
    "\n",
    "x_val, y_val = df_validate, df_validate[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shapes:\\nx_train:%s\\ny_train:%s\\n\" % (x_train.shape, y_train.shape))\n",
    "print(\"x_test:%s\\ny_test:%s\\n\" % (x_test.shape, y_test.shape))\n",
    "print(\"x_val:%s\\ny_val:%s\\n\" % (x_val.shape, y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert your model code here; call your model \"model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_scores = model.decision_function(x_val)\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.hist(anomaly_scores, bins=100)\n",
    "plt.xlabel('Average Path Lengths', fontsize=14)\n",
    "plt.ylabel('Number of Data Points', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "anomalies = anomaly_scores > -0.19\n",
    "matches = y_val == list(encoded.classes_).index(\"normal.\")\n",
    "auc = roc_auc_score(anomalies, matches)\n",
    "print(\"AUC: {:.2%}\".format (auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_scores_test = model.decision_function(x_test)\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.hist(anomaly_scores_test, bins=100)\n",
    "plt.xlabel('Average Path Lengths', fontsize=14)\n",
    "plt.ylabel('Number of Data Points', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_test = anomaly_scores_test > -0.19\n",
    "matches = y_test == list(encoded.classes_).index(\"normal.\")\n",
    "auc = roc_auc_score(anomalies_test, matches)\n",
    "print(\"AUC: {:.2%}\".format (auc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
